{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98e1a57",
   "metadata": {},
   "source": [
    "# ACTIVITY: A CLASSIFIER\n",
    "\n",
    "The goal of this activity is to practice building and discussing a classifier. By the end of the activity, you should be able to justify your design decisions according to \n",
    "\n",
    "TASK:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af144335",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b490f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.11).\n",
      "['bbc-news-data.csv']\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = kagglehub.dataset_download(\"hgultekin/bbcnewsarchive\")\n",
    "print(os.listdir(path))\n",
    "df = pd.read_csv(Path(path) / \"bbc-news-data.csv\", sep='\\t').sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d06ea01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>400.txt</td>\n",
       "      <td>US cyber security chief resigns</td>\n",
       "      <td>The man making sure US computer networks are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>tech</td>\n",
       "      <td>135.txt</td>\n",
       "      <td>GTA sequel is criminally good</td>\n",
       "      <td>The Grand Theft Auto series of games have set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>business</td>\n",
       "      <td>248.txt</td>\n",
       "      <td>Survey confirms property slowdown</td>\n",
       "      <td>Government figures have confirmed a widely re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>021.txt</td>\n",
       "      <td>Obituary: Dame Alicia Markova</td>\n",
       "      <td>Dame Alicia Markova, who has died in Bath age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>089.txt</td>\n",
       "      <td>Oscar nominee Dan O'Herlihy dies</td>\n",
       "      <td>Irish actor Dan O'Herlihy, who was nominated ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category filename                              title  \\\n",
       "2223           tech  400.txt    US cyber security chief resigns   \n",
       "1958           tech  135.txt      GTA sequel is criminally good   \n",
       "247        business  248.txt  Survey confirms property slowdown   \n",
       "530   entertainment  021.txt      Obituary: Dame Alicia Markova   \n",
       "598   entertainment  089.txt   Oscar nominee Dan O'Herlihy dies   \n",
       "\n",
       "                                                content  \n",
       "2223   The man making sure US computer networks are ...  \n",
       "1958   The Grand Theft Auto series of games have set...  \n",
       "247    Government figures have confirmed a widely re...  \n",
       "530    Dame Alicia Markova, who has died in Bath age...  \n",
       "598    Irish actor Dan O'Herlihy, who was nominated ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5fbe320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "tech             2\n",
       "entertainment    2\n",
       "business         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423b8dd",
   "metadata": {},
   "source": [
    "## Baseline classifier\n",
    "\n",
    "We will start with a baseline classifier. It is a simple Bag-of-words classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b480959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "entertainment       0.00      0.00      0.00       0.0\n",
      "         tech       0.00      0.00      0.00       1.0\n",
      "\n",
      "     accuracy                           0.00       1.0\n",
      "    macro avg       0.00      0.00      0.00       1.0\n",
      " weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211af15f",
   "metadata": {},
   "source": [
    "## PART 1: preparation\n",
    "\n",
    "Answer the questions below as comments in the following cell. In your answers, avoid using common sense. Use adequate jargons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a4b79",
   "metadata": {},
   "source": [
    "### Question 1: What is the underlying premise of the Bag-of-Words classifier, that is, why does BoW allow to classify these texts?\n",
    "\n",
    "The underying premise of BoW is to create a document x words matrix, which each cell represents the frequency of a word in a text. Thus, is it possible get to the importance of each word in a document, so the weight of a classification. \n",
    "Therefore, the classify model uses \n",
    "\n",
    "frequencia das palavras sem contexto/ordem\n",
    "\n",
    "importancia das palavras dado topico (numericamente: pesos)\n",
    "\n",
    "usar modelo de classificacao com base nos pesos gerados pelo bow\n",
    "\n",
    "legibilidade fácil  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97188304",
   "metadata": {},
   "source": [
    "### Question 2: What is the underlying premise of a BERT-based classifier, that is, why should BERT embeddings be interesting to classify these texts?\n",
    "\n",
    "BERT pretreinado para \n",
    "\n",
    "BERT embeddings are representation of word "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4752c77",
   "metadata": {},
   "source": [
    "## PART 2: action\n",
    "\n",
    "(a) Make a classifier that uses BERT embeddings to categorize the texts in the dataset we have discussed.\n",
    "\n",
    "(b) Make a bar plot comparing the accuracy of the BERT-based classifier to that of the Bag-of-Words classifier\n",
    "\n",
    "(c) Use a PCA or a T-SNE plot to visualize the documents in the newsgroups dataset in the embedding space provided by BERT. Analyze the plot taking into account the confusion matrix or the classification report of your BERT-based classifier.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c1e5e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 4/4 [00:01<00:00,  3.13it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2671\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[0;32m   2564\u001b[0m \n\u001b[0;32m   2565\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2667\u001b[0m \u001b[38;5;124;03m<BLANKLINE>\u001b[39;00m\n\u001b[0;32m   2668\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2670\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[1;32m-> 2671\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2674\u001b[0m     labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m---> 98\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 5]"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm   \n",
    "\n",
    "\n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased') \n",
    "        self.embeddings = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "  \n",
    "    def transform(self, texts):\n",
    "        for text in tqdm(texts, desc=\"Generating embeddings\"):\n",
    "              inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length)\n",
    "              outputs = self.model(**inputs)\n",
    "              cls_embedding = outputs.last_hidden_state[0, 0, :] # embedding que corresponde ao texto\n",
    "              self.embeddings.append(cls_embedding.detach().numpy())\n",
    "        return self.embeddings\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bert', BertTransformer()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5745bc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertTransformer' object has no attribute 'embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\n\u001b[0;32m      6\u001b[0m e_pca \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertTransformer' object has no attribute 'embeddings'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(2)\n",
    "embeddings = pipeline.named_steps[\"bert\"].embeddings\n",
    "e_pca = pca.fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1805f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595b2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
