{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4e4871",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "**GOAL: At the end of this class, we will be able to USE a pre-trained BERT to (a) generate suggestions and to (b) generate embeddings for classification**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e01af4e",
   "metadata": {},
   "source": [
    "## What is BERT?\n",
    "\n",
    "After the [transformer](https://arxiv.org/abs/1706.03762), we had many other advances. One of such, of course, is the [GPT](https://paperswithcode.com/paper/improving-language-understanding-by), which uses a decoder-only transformer architecture to predict the next word in a sentence. GPT uses a decoder-only architecture because it needs the masked multi-head attention device to avoid making trivial predictions. Ultimately, GPT generates an embedding space that increases the likelihood of choosing meaningful words for a text continuation.\n",
    "\n",
    "The Google team found another interesting way to obtain this type of representation. They trained an *encoder*-only transformer that can predict words removed from the text - similarly to how we know what is missing in  \"Luke, I am your ____\". The idea here is that we can use information from the future for this task, because it is highly dependent on context. Simultaneously, they trained the model to classify whether two given phrases follow each other in a corpus. So, BERT was born."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00a497",
   "metadata": {},
   "source": [
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    subgraph Input;\n",
    "    T[\"Token embeddings\"];\n",
    "    P[\"Position embeddings\"];\n",
    "    S[\"Segment embeddings \n",
    "    (indicates if it is sentence 1\n",
    "     or sentence 2 in NSP task)\"];\n",
    "    ADD([\"\\+\"]);\n",
    "    T --> ADD;\n",
    "    P --> ADD;\n",
    "    S --> ADD; \n",
    "    end;\n",
    "\n",
    "    SEQ[\"Sequence Model\"];\n",
    "    ADD --> SEQ;\n",
    "    RES[\"Result: 1 vector per input token\"];\n",
    "    SEQ --> RES;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ef601",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Bert stands for [Bidirectional Encoder Representations from Transformers, and was introduced in this paper from 2019](https://arxiv.org/pdf/1810.04805). The greatest contribution of BERT, besides its architecture, is the idea of training the language model for different tasks at the same time.\n",
    "\n",
    "We are definitely not going to train BERT in class, but we are using it for other tasks. We will use the [BERT implementation from Hugging Face](https://huggingface.co/google-bert/bert-base-uncased). All help files are here.\n",
    "\n",
    "## Task 1: Masked Language Model\n",
    "\n",
    "The first task BERT was trained for was the Masked Language Model. This was inspired in a task called [\"Cloze\"](https://en.wikipedia.org/wiki/Cloze_test), and the idea is to remove a word from a sentence and let the system predict what word should fill that sentence:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211077b9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    subgraph Inputs;\n",
    "    INPUT[\"[CLS]\n",
    "        remove\n",
    "        some\n",
    "        parts\n",
    "        [MASK]\n",
    "        a\n",
    "        sentence\"];\n",
    "    end;\n",
    "    INPUT --> BERT[\"BERT\"];\n",
    "    subgraph Outputs;\n",
    "    OUTPUT[\"C\n",
    "    T1\n",
    "    T2\n",
    "    T3\n",
    "    T4\n",
    "    T5\n",
    "    T6\"];\n",
    "    end;\n",
    "    BERT --> OUTPUT;\n",
    "    Train[\"Loss: T4 should be the word 'of'\"]\n",
    "    OUTPUT --- Train;\n",
    "```\n",
    "\n",
    "\n",
    "This task suggests that the embedding space created by BERT should allow representing words in the context of the rest of the sentence!\n",
    "\n",
    "To play with this task with Hugging Face's library, you can use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f66818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5548137b3c63491faf6eb2ef1ae428e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  26%|##6       | 115M/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b9662690bb40148a6599c8410af064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45d44564a264d0a81f7bf5c6959f19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  26%|##6       | 115M/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1218bcbaff4b8d83887af3af3f1975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model bert-base-uncased with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForMaskedLM'>, <class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>). See the original errors:\n\nwhile loading with AutoModelForMaskedLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 571, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 279, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4260, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 1080, in _get_resolved_checkpoint_files\n    raise EnvironmentError(\nOSError: bert-base-uncased does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\nwhile loading with BertForMaskedLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 279, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4260, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 1080, in _get_resolved_checkpoint_files\n    raise EnvironmentError(\nOSError: bert-base-uncased does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 2\u001b[0m unmasker \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfill-mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m unmasker(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemove some parts [MASK] a sentence.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:942\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 942\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    952\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    953\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\pipelines\\base.py:304\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    303\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         )\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model bert-base-uncased with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForMaskedLM'>, <class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>). See the original errors:\n\nwhile loading with AutoModelForMaskedLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 571, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 279, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4260, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 1080, in _get_resolved_checkpoint_files\n    raise EnvironmentError(\nOSError: bert-base-uncased does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\nwhile loading with BertForMaskedLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 279, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4260, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 1080, in _get_resolved_checkpoint_files\n    raise EnvironmentError(\nOSError: bert-base-uncased does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Remove some parts [MASK] a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf1f3f",
   "metadata": {},
   "source": [
    "### Algorithmic bias and Hallucinations\n",
    "\n",
    "Note that BERT is generating words that make sense. However, these continuations do not necessarily correspond to reality. In fact, these continuations are simply something that maximizes a probability related to a specific dataset!\n",
    "\n",
    "Check, for example, the output for:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe9a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07573200762271881,\n",
       "  'token': 4511,\n",
       "  'token_str': 'wine',\n",
       "  'sequence': 'kentucky is famous for its wine.'},\n",
       " {'score': 0.06742826849222183,\n",
       "  'token': 14746,\n",
       "  'token_str': 'wines',\n",
       "  'sequence': 'kentucky is famous for its wines.'},\n",
       " {'score': 0.02818026952445507,\n",
       "  'token': 12212,\n",
       "  'token_str': 'beaches',\n",
       "  'sequence': 'kentucky is famous for its beaches.'},\n",
       " {'score': 0.022783828899264336,\n",
       "  'token': 12846,\n",
       "  'token_str': 'cuisine',\n",
       "  'sequence': 'kentucky is famous for its cuisine.'},\n",
       " {'score': 0.021198133006691933,\n",
       "  'token': 5194,\n",
       "  'token_str': 'horses',\n",
       "  'sequence': 'kentucky is famous for its horses.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Kentucky is famous for its [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f1899",
   "metadata": {},
   "source": [
    "\n",
    "Kentucky is a state in the USA that may or may not have wineries, but definitely does not have famous beaches! Now, check the output when you change Kentucky for the Brazilian state of Minas Gerais!\n",
    "\n",
    "See - there is no \"brain\" inside BERT. There is merely a system that finds plausible completions for a task. This is something we have been calling \"hallucinations\" in LLMs. In the end, the model is just as biased as the dataset used for training it.\n",
    "\n",
    "### Algorithmic prejudice\n",
    "\n",
    "Despite the funny things things that the model could output, there are some assertions that can be dangerous, or outright sexist. Try to see the output of:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77017a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.06930015236139297,\n",
       "  'token': 3460,\n",
       "  'token_str': 'doctor',\n",
       "  'sequence': 'a successful man works as a doctor.'},\n",
       " {'score': 0.06541887670755386,\n",
       "  'token': 5160,\n",
       "  'token_str': 'lawyer',\n",
       "  'sequence': 'a successful man works as a lawyer.'},\n",
       " {'score': 0.04109674692153931,\n",
       "  'token': 7500,\n",
       "  'token_str': 'farmer',\n",
       "  'sequence': 'a successful man works as a farmer.'},\n",
       " {'score': 0.03909466415643692,\n",
       "  'token': 10533,\n",
       "  'token_str': 'carpenter',\n",
       "  'sequence': 'a successful man works as a carpenter.'},\n",
       " {'score': 0.03854001685976982,\n",
       "  'token': 22701,\n",
       "  'token_str': 'tailor',\n",
       "  'sequence': 'a successful man works as a tailor.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"A successful man works as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker(\"A successful woman works as a [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af373066",
   "metadata": {},
   "source": [
    "\n",
    "Now, change \"man\" for \"woman\". The result is not as pretty. But, see, this is not a problem of the language model structure per se - rather, it is a problem of the data used to train it.\n",
    "\n",
    "We could go on finding examples of other types of prejudice - there are all sorts of sexism and racism lying in the hidden spaces of BERT.\n",
    "\n",
    "This is bad, but remember this was 2019, and people were impressed that the system could generate coherent words at all! Nowadays, LLM outputs go through a filter that finds phrases that are potentially harmful, so they don't write ugly phrases.\n",
    "\n",
    "Which of the phrases below are true about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa293a1",
   "metadata": {},
   "source": [
    "## Task 2: Next Sentence Prediction\n",
    "\n",
    "BERT was also trained for a task called Next Sentence Prediction. The idea of this task is to insert two sentences in the input of BERT, separating them with a special [SEP] token. Then, the system uses the output of the [CLS] token to classify whether these two sentences do or do not follow each other. It is something like:\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    subgraph Inputs;\n",
    "    INPUT[\"[CLS]\n",
    "        Here\n",
    "        I\n",
    "        am\n",
    "        [SEP]\n",
    "        rock\n",
    "        you\n",
    "        like\n",
    "        a\n",
    "        hurricane\"];\n",
    "    end;\n",
    "    INPUT --> BERT[\"BERT\"];\n",
    "    subgraph Outputs;\n",
    "    OUTPUT[\"C\n",
    "    T1\n",
    "    T2\n",
    "    etc\"];\n",
    "    end;\n",
    "    BERT --> OUTPUT;\n",
    "    Train[\"Loss: C should be equal to 1\"]\n",
    "    OUTPUT --- Train;\n",
    "```\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    subgraph Inputs;\n",
    "    INPUT[\"[CLS]\n",
    "        Here\n",
    "        I\n",
    "        am\n",
    "        [SEP]\n",
    "        rock\n",
    "        your\n",
    "        body\"];\n",
    "    end;\n",
    "    INPUT --> BERT[\"BERT\"];\n",
    "    subgraph Outputs;\n",
    "    OUTPUT[\"C\n",
    "    T1\n",
    "    T2\n",
    "    etc\"];\n",
    "    end;\n",
    "    BERT --> OUTPUT;\n",
    "    Train[\"Loss: C should be equal to 0\"]\n",
    "    OUTPUT --- Train;\n",
    "```\n",
    "\n",
    "The consequence of this training is that the embedding $C$ of the [CLS] token represents the content of the rest of the tokens. Hence, we can use it for classification. For such, we can go straight to the HuggingFace library and use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1fb9a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfb9dc3de7b494d82c27119b9212893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  26%|##6       | 115M/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef1f58c47484fedbf486d8c6f0498b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "bert-base-uncased does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReplace me by any text you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md like.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py:4260\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4251\u001b[0m     gguf_file\n\u001b[0;32m   4252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4253\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[0;32m   4254\u001b[0m ):\n\u001b[0;32m   4255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   4256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4258\u001b[0m     )\n\u001b[1;32m-> 4260\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4262\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4267\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4273\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4278\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4279\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alema\\OneDrive\\Documentos\\insper\\7sem\\nlp_course\\env\\Lib\\site-packages\\transformers\\modeling_utils.py:1080\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[1;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[0;32m   1072\u001b[0m has_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m: revision,\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxies\u001b[39m\u001b[38;5;124m\"\u001b[39m: proxies,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: local_files_only,\n\u001b[0;32m   1078\u001b[0m }\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[1;32m-> 1080\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1081\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for TensorFlow weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `from_tf=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1084\u001b[0m     )\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1087\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1088\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for Flax weights. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `from_flax=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: bert-base-uncased does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1dac23",
   "metadata": {},
   "source": [
    "\n",
    "The embedding for the [CLS] token can be accessed using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "750e786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cls = output.last_hidden_state[0,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a9cb9",
   "metadata": {},
   "source": [
    "    \n",
    "There are many details in this implementation, so I made a [video exploring them all](https://youtu.be/FXtGq_TYLzM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd4017",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Our usual way to approach classification is to do something in the lines of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee984b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv').sample(1000)\n",
    "X = df['Plot']\n",
    "y = df['Genre']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Test the pipeline\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5876be",
   "metadata": {},
   "source": [
    "Now, instead of using a TfIdf vectorizer, calculate embeddings for the texts in the dataset using BERT. Then, use *them* to classify. Compare the results with the ones we have when we use the Bag-of-words approach.\n",
    "\n",
    "Justify these results using the concept of embeddings we have studied in the previous lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f634d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your solution here\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv').sample(1000)\n",
    "X = df['Plot']\n",
    "y = df['Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f046e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bb02d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:35<00:00,  3.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# This is my solution - do not copy it!\n",
    "\n",
    "# Step 0: get data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv').sample(1000)\n",
    "X = df['Plot']\n",
    "y = df['Genre']\n",
    "\n",
    "# Step 1: preprocess the text\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "def get_embeddings(text, model, tokenizer):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "embeddings = []\n",
    "for i in tqdm(range(len(X))):\n",
    "    e = get_embeddings(X.iloc[i], model, tokenizer)\n",
    "    embeddings.append(e.detach().numpy())\n",
    "embeddings = np.array(embeddings)\n",
    "np.save('bert_embeddings.npy', embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b709b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      comedy       0.74      0.75      0.74        79\n",
      "       drama       0.83      0.83      0.83       121\n",
      "\n",
      "    accuracy                           0.80       200\n",
      "   macro avg       0.79      0.79      0.79       200\n",
      "weighted avg       0.80      0.80      0.80       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.load('bert_embeddings.npy')\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, random_state=42)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa1ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd94d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
